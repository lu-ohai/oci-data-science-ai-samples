{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "standard-throat",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=red>Multi-label classification with nltk and scikit-learn</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle Cloud Infrastructure Data Science Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-criticism",
   "metadata": {},
   "source": [
    "<font color=gray>ADS Sample Notebook.\n",
    "\n",
    "Copyright (c) 2021 Oracle, Inc.  All rights reserved.\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-penetration",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook shows you how to develop a multi-label text classification system on the Reuters Corpus. The skills taught in this notebook are applicable to a wide variety of tasks. Multi-label classification is not significantly more difficult than single-label classification. It does require some slightly different techniques, which are shown in this notebook. \n",
    "\n",
    "We use `scikit-learn` and `nltk` to build an effective multi-label classifier in minutes. We use the [Reuters Corpus](https://martin-thoma.com/nlp-reuters/) as our training dataset.\n",
    "\n",
    "Thom2017-reuters,\n",
    "  Title                    = {The Reuters Dataset},\n",
    "\n",
    "  Author                   = {Martin Thoma},\n",
    "  Month                    = jul,\n",
    "  Year                     = {2017},\n",
    "\n",
    "  Url                      = {https://martin-thoma.com/nlp-reuters}\n",
    "}\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-athens",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    " - Experience with the topic: Novice\n",
    " - Professional experience: None\n",
    " \n",
    "This notebook is intended for Data Scientists with desire to learn about Natural Language Processing tasks and experienced Data Sciencests who want to add another tool to their toolbox.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-tomato",
   "metadata": {},
   "source": [
    "### First, import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-tonight",
   "metadata": {},
   "source": [
    "Next, download the dataset and the list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-olympus",
   "metadata": {},
   "source": [
    "The distributor of the reuters dataset also graciously released their code for loading the data to the public. We utilize it with slight modifications. \n",
    "\n",
    "Reuters is a benchmark dataset for document classification. To be more precise, it is a multi-label (each document can belong to many classes) dataset. It has 90 classes, 7769 training documents, and 3019 testing documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-nurse",
   "metadata": {},
   "source": [
    "During the dataset loading process, we utilize the `MultiLabelBinarizer()` method for converting the labels present originally into the format that scikit-learn wants for doing classification.  This transformer converts between a list of sets or tuples and the supported multi-label format, which is a (samples x classes) binary matrix indicating the presence of a class label. Further details about how this works can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html). \n",
    "\n",
    "\n",
    "Once you figure out how to encode the labels, you have to vectorize the text to make it possible for supervised machine learning systems to learn from. One of the most popular and effective strategies for this is called `tf-idf`, which is a vectorization technique that weighs a termâ€™s frequency (tf) and its inverse document frequency (idf). Each word or term that occurs in the text has its respective tf and idf score. Putting them together gives us the `tf-idf` score. Intuitively, a higher score corresponds to a tokens being more \"important\". Words like \"the\" have a high term frequency, but a low inverse document frequency because they are utilized everywhere in the corpus. The word \"the\" would get a low `tf-idf` score. A specific word like \"whale\" may be utilized very seldomly through the corpus, giving it a high inverse document score and a high term frequency score in the few documents that are about it. As a result, it would get a very high `tf-idf` score. More details about `tf-idf` can be found [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "We limit the `TfidfVectorizer` to only 10000 words for performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = reuters.categories()\n",
    "def load_data(config={}):\n",
    "    \"\"\"\n",
    "    Load the Reuters dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "    labels = reuters.categories()\n",
    "    stop_words = stopwords.words(\"english\") ## See scikit-learn documentation for what these words are\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 10000)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    documents = reuters.fileids()\n",
    "    test = [d for d in documents if d.startswith(\"test/\")] ##Get the locations for the training documents\n",
    "    train = [d for d in documents if d.startswith(\"training/\")] ##Get the locations for the testing documents \n",
    "\n",
    "    docs = {}\n",
    "    docs[\"train\"] = [reuters.raw(doc_id) for doc_id in train] ##Populate the list with the docs\n",
    "    docs[\"test\"] = [reuters.raw(doc_id) for doc_id in test]\n",
    "    xs = {\"train\": [], \"test\": []}\n",
    "    xs[\"train\"] = vectorizer.fit_transform(docs[\"train\"]).toarray() ##Vectorize the inputs with tf-idf \n",
    "    xs[\"test\"] = vectorizer.transform(docs[\"test\"]).toarray()\n",
    "    ys = {\"train\": [], \"test\": []}\n",
    "    ys[\"train\"] = mlb.fit_transform([reuters.categories(doc_id) for doc_id in train]) ##Vectorize the labels \n",
    "    ys[\"test\"] = mlb.transform([reuters.categories(doc_id) for doc_id in test])\n",
    "    data = {\n",
    "        \"x_train\": xs[\"train\"],\n",
    "        \"y_train\": ys[\"train\"],\n",
    "        \"x_test\": xs[\"test\"],\n",
    "        \"y_test\": ys[\"test\"],\n",
    "        \"labels\": globals()[\"labels\"],\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-heaven",
   "metadata": {},
   "source": [
    "You can now load the data easily into a format ready for scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reuters_data['x_train']\n",
    "y = reuters_data['y_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-botswana",
   "metadata": {},
   "source": [
    "To properly support multi-label problems, you must use a `OnevsRestClassifier`. More details about the reasoning for this can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier).\n",
    "\n",
    "We choose the `LinearSVC` model because it is very fast to train and empiraclly effective on NLP problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(LinearSVC(class_weight = \"balanced\"), n_jobs = -1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-allah",
   "metadata": {},
   "source": [
    "Let's see how the model did! You utilize cross validation, a common statistical technique to convince us that our model properly generalizes with a certain performance. K fold cross-validation works by partitioning a dataset into K splits, performing the analysis on one training set, and validating on another smaller data split. For more details about this process, look [here](https://en.wikipedia.org/wiki/Cross-validation_(statistics) and specifically at this image [here](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/1920px-K-fold_cross_validation_EN.svg.png)\n",
    "\n",
    "By performing cross validation, you get 5 seperate models trained on different train and test splits of the dataset. If you average these scores, you can get a pretty good repersentation of how a model may perform \"in the wild\" on unseen data. As always, it's not a guarantee of good performance in the future, but it's considered by many to be the gold standard of model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-mexican",
   "metadata": {},
   "source": [
    "A pretty robust performance! You have made an effective multi-label text classifier over the Reuters Corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_gpu_conda_final_versionv1_0]",
   "language": "python",
   "name": "conda-env-nlp_gpu_conda_final_versionv1_0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
