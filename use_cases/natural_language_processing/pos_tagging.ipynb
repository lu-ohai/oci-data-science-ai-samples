{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "combined-hearts",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-convergence",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=red>Part of Speech tagging with nltk and scikit-learn</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle Cloud Infrastructure Data Science Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-plane",
   "metadata": {},
   "source": [
    "<font color=gray>ADS Sample Notebook.\n",
    "\n",
    "Copyright (c) 2021 Oracle, Inc.  All rights reserved.\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-runner",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook demo will show how to develop a token classification system for tagging the part of speech that a word is. The skills taught in this notebook are applicable to other problems, like named entity recognition.\n",
    "\n",
    "We use `scikit-learn` and `nltk` to build an effective POS classifier in seconds. We use the Penn `treebank` corpus as our training dataset. More specific information about what a treebank is can be found [here](https://en.wikipedia.org/wiki/Treebank)\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-seattle",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    " - Experience with the topic: Novice\n",
    " - Professional experience: None\n",
    " \n",
    "This notebook is intended for Data Scientists with desire to learn about Natural Language Processing tasks as well as experienced Data Sciencests who want to add another tool to their toolbox\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-chapter",
   "metadata": {},
   "source": [
    "### First, import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-facing",
   "metadata": {},
   "source": [
    "We will download the treebank dataset - this may take awhile because the treebank dataset consists of 3914 tagged sentences and 100676 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-bubble",
   "metadata": {},
   "source": [
    "We load the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-preservation",
   "metadata": {},
   "source": [
    "Here is an example of a sentence and its part of speech tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-sellers",
   "metadata": {},
   "source": [
    "The various parts of speech codes used in the treebank dataset and their corresponding description are as follows: \n",
    "\n",
    "\n",
    "1. \tCC \tCoordinating conjunction\n",
    "2. \tCD \tCardinal number\n",
    "3. \tDT \tDeterminer\n",
    "4. \tEX \tExistential there\n",
    "5. \tFW \tForeign word\n",
    "6. \tIN \tPreposition or subordinating conjunction\n",
    "7. \tJJ \tAdjective\n",
    "8. \tJJR \tAdjective, comparative\n",
    "9. \tJJS \tAdjective, superlative\n",
    "10. \tLS \tList item marker\n",
    "11. \tMD \tModal\n",
    "12. \tNN \tNoun, singular or mass\n",
    "13. \tNNS \tNoun, plural\n",
    "14. \tNNP \tProper noun, singular\n",
    "15. \tNNPS \tProper noun, plural\n",
    "16. \tPDT \tPredeterminer\n",
    "17. \tPOS \tPossessive ending\n",
    "18. \tPRP \tPersonal pronoun\n",
    "19. \tPRP \tPossessive pronoun\n",
    "20. \tRB \tAdverb\n",
    "21. \tRBR \tAdverb, comparative\n",
    "22. \tRBS \tAdverb, superlative\n",
    "23. \tRP \tParticle\n",
    "24. \tSYM \tSymbol\n",
    "25. \tTO \tto\n",
    "26. \tUH \tInterjection\n",
    "27. \tVB \tVerb, base form\n",
    "28. \tVBD \tVerb, past tense\n",
    "29. \tVBG \tVerb, gerund or present participle\n",
    "30. \tVBN \tVerb, past participle\n",
    "31. \tVBP \tVerb, non-3rd person singular present\n",
    "32. \tVBZ \tVerb, 3rd person singular present\n",
    "33. \tWDT \tWh-determiner\n",
    "34. \tWP \tWh-pronoun\n",
    "35. \tWP \tPossessive wh-pronoun\n",
    "36. \tWRB \tWh-adverb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-intervention",
   "metadata": {},
   "source": [
    "We need to write a function to take in these tagged sentences and return a feature dictionary for that particular sentence. scikit-learn's offical docs have more details about this process [here](https://scikit-learn.org/stable/modules/feature_extraction.html#loading-features-from-dicts)\n",
    "\n",
    "One could also use word embeddings from models like `word2vec` - but each vector component must be included as an independent dictionary key/value. `scikit-learn` doesn't support storing a whole numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tagged_sentence, index):\n",
    "    token, tag = tagged_sentence[index]\n",
    "    prev_token = \"\"\n",
    "    if index > 0:\n",
    "        prev_token, prev_tag = tagged_sentence[index - 1]\n",
    "    is_number = False\n",
    "    try:\n",
    "        if float(token):\n",
    "            is_number = True\n",
    "    except:\n",
    "        pass\n",
    "    features_dict = {\"token\": token\n",
    "        , \"lower_cased_token\": token.lower()\n",
    "        , \"prev_token\": prev_token\n",
    "        , \"suffix1\": token[-1]\n",
    "        , \"suffix2\": token[-2:]\n",
    "        , \"suffix3\": token[-3:]\n",
    "        , \"prefix1\": token[:1]\n",
    "        , \"prefix2\": token[:2]\n",
    "        , \"prefix3\": token[:3]\n",
    "        , \"is_capitalized\": token.upper() == token\n",
    "        , \"is_number\": is_number}\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-image",
   "metadata": {},
   "source": [
    "Here is what the output of this feature extractor looks like on a part our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features(tagged_sentences[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-blank",
   "metadata": {},
   "source": [
    "We can now use our function to generate our input data. This is a performance intensive task, and to keep it easy to run this notebook on small shapes, we downsample to only the first 100 sentences. One is free to remove this downsampling amd they will find that the model performs more effectively but takes far longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = []\n",
    "for sentence in tagged_sentences[0:100]:\n",
    "    for k in range(len(sentence)):\n",
    "        X_features.append(extract_features(sentence, k))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-contest",
   "metadata": {},
   "source": [
    "We need to use a `DictVectorizer` to convert from our dictionary repersentation into data that `scikit-learn` understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = DictVectorizer(sparse=False)\n",
    "X = vectoriser.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-fitting",
   "metadata": {},
   "source": [
    "We need our labels (the POS tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for sentence in tagged_sentences[0:100]:\n",
    "    for k in range(len(sentence)):\n",
    "        y.append(sentence[k][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-convenience",
   "metadata": {},
   "source": [
    "We can now train a model to do POS tagging on this corpus. POS tagging is compute intensvie so we choose the speedy `SGDClassifier`. For even larger datasets, one may want to try the `LinearSVC` model with careful hyperparamater choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(n_jobs = -1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-extreme",
   "metadata": {},
   "source": [
    "Finally, we can perform cross validation to get an idea of the performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-given",
   "metadata": {},
   "source": [
    "Let's print the average to see the performance that we might estimate to get in the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-consequence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_conda_pack_real]",
   "language": "python",
   "name": "conda-env-nlp_conda_pack_real-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
